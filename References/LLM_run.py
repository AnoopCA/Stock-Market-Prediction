import torch
import sys

sys.path.append(r'D:\ML_Projects\Stock-Market-Prediction\Scripts')
# Import the GPT2 model created
from llm_model import Head, MultiHeadAttention, FeedFoward, Block, GPTLanguageModel

device = 'cuda' if torch.cuda.is_available() else 'cpu'

#Load the pre-trained model to generate text
model = torch.load(r'D:\ML_Projects\Stock-Market-Prediction\Models\model-25_loss-8.943.pth')

model.eval()
model.to(device)

# Run the model on test data
prompt = [16915, 16874, 16881, 16880, 16885, 16877, 16868, 16866, 16852, 16856, 16856, 16846, 16849, 16849, 16848, 16852, 16845, 16838, 16832, 16815, 16809, 16816, 16816, 16813, 16819, 16807, 16811, 16806, 16810, 16820, 16814, 16823, 16818, 16817, 16820, 16812, 16808, 16796, 16799, 16803, 16803, 16794, 16788, 16782, 16788, 16791, 16791, 16784, 16782, 16793, 16804, 16805, 16806, 16802, 16794, 16786, 16793, 16787, 16776, 16782, 16780, 16781, 16783, 16785, 16785, 16788, 16796, 16791, 16787, 16791, 16801, 16800, 16799, 16792, 16787, 16786, 16792, 16797, 16788, 16783, 16787, 16787, 16793, 16797, 16801, 16803, 16805, 16804, 16799, 16801, 16800, 16801, 16796, 16790, 16788, 16785, 16795, 16799, 16796, 16791, 16789, 16791, 16793, 16794, 16797, 16795, 16796, 16787, 16782, 16784, 16784, 16769, 16771, 16770, 16773, 16771, 16772, 16771, 16763, 16767, 16770, 16764, 16763, 16766, 16760, 16752, 16757, 16761, 16759, 16756, 16759, 16764, 16758, 16760, 16761, 16759, 16755, 16761, 16757, 16759, 16761, 16766, 16766, 16767, 16766, 16762, 16763, 16760, 16759, 16761, 16764, 16769, 16771, 16770, 16777, 16769, 16770, 16766, 16763, 16761, 16760, 16760, 16761, 16758, 16761, 16762, 16767, 16766, 16767, 16764, 16763, 16767, 16766, 16766, 16767, 16767, 16770, 16770, 16773, 16773, 16773, 16776, 16775, 16771, 16769, 16768, 16769, 16766, 16774, 16774, 16779, 16775, 16773, 16774, 16770, 16766, 16762, 16756, 16758, 16759, 16758, 16752, 16754, 16753, 16759, 16764, 16760, 16766, 16765, 16758, 16760, 16759, 16758, 16754, 16757, 16753, 16750, 16742, 16742, 16742, 16742, 16743, 16746, 16740, 16734, 16735, 16737, 16736, 16735, 16736, 16735, 16739, 16737, 16740, 16737, 16736, 16739, 16736, 16738, 16737, 16741, 16736, 16739, 16737, 16744, 16742, 16741, 16741, 16743, 16748, 16750, 16746, 16748, 16753, 16753, 16748, 16745, 16746, 16746, 16744, 16749, 16748, 16741, 16743, 16739, 16743, 16744, 16742, 16738, 16738, 16741, 16734, 16729, 16731, 16731, 16727, 16725, 16719, 16718, 16718, 16722, 16721, 16737, 16740, 16741, 16733, 16743, 16745, 16745, 16743, 16743, 16737, 16738, 16734, 16733, 16735, 16730, 16731, 16729, 16725, 16726, 16730, 16729, 16725, 16726, 16724, 16733, 16727, 16725, 16724, 16724, 16723, 16724, 16726, 16726, 16727, 16723, 16715, 16717, 16717, 16720, 16721, 16723, 16726, 16731, 16727, 16729, 16736, 16733, 16734, 16728, 16726, 16730, 16729, 16726, 16727, 16724, 16725, 16723, 16726, 16725, 16727, 16730, 16729, 16729, 16735, 16733, 16737, 16748, 16749, 16757, 16759, 16763, 16757, 16761, 16761, 16758, 16751, 16749, 16741, 16749, 16746, 16760, 16763, 16764, 16768, 16768, 16772, 16778, 16772, 16770, 16773, 16768, 16780, 16779]
context = torch.tensor(prompt, dtype=torch.long, device=device).unsqueeze(0)
generated_data = model.generate(context, max_new_tokens=375)[0].tolist()
generated_data = generated_data[375:]
print(generated_data)
